\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[russian]{babel} %comment it for english!

\usepackage{amsfonts,longtable,amssymb,amsmath,array}
\usepackage{euscript}
\usepackage[xetex]{graphicx}
%\usepackage{fontspec,xunicode}
\usepackage{mathspec}
\usepackage{enumitem}
%\defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
%\fontspec{Courier New}
\setallmainfonts{Monaco}
%\setmonofont{Monaco}
\renewcommand{\theenumi}{\Alph{enumi}}
\newcommand{\suml}[0]{\sum\limits}
\begin{document}
\section{Теория}
\subsection{Задача}
Так как тестовая выборка линейно разделима, то обученный на ней SVM
не будет иметь ошибок ($N = 0$), поэтому вес алгоритма $a_t = \frac{1}{2}\log(\frac{1-N}{N}) = +\infty$
примерно.

\subsection{Задача}
Веса объектов в алгоритме AnyBoost вычисляются как значения производной функции отступа, взятых со знаком минус. Функция отступа невозрастает, производная неположительна.

\subsection{Задача}
Вроде decision stump алгоритмы независимы по признаку, поэтому сразу умножим сложность поиска для фиксированного признака на $n$. 
Пусть $X = \{x_1, ..., x_l\}$ - множество различных значений признака, $|X| \le l$ вообще говоря. Тогда ясно, что параметр $\theta$
существенно меняет алгоритм при переходе через значения в $X$. Таких переходов $O(l)$, число ошибок можно пересчитывать за $O(1)$ на ходу.
На сортировку нужно $O(n\log(n))$. Всего $O(ln\log(l))$. Интересно, как можно лучше?

\subsection{Задача}
Если веса рациональны, то можно неоптимально сильно увеличить выборку, взять каждого объекта по несколько штук, чтобы нормированные
количества совпали с нормированными весами $W$. Если вещественны, то сделаем аппроксимацию рациональными и применим неоптимальное размножение объектов. Интересно, как можно лучше?

\subsection{Задача}
Зафиксируем распределение меток, пусть '+' и '-'. Разобьем упорядоченную последовательность чисел с метками на последовательные блоки
из чисел с одинаковыми метками, примерно так, как делает groupby в python с предикатом равенства меток. Ясно, что таких блоков
есть $n = O(l)$. Как угодно разделим соседние блоки, направляя вектор нормали плоскости в сторону фиксированной метки, скажем '+'.
Для каждого блока вычислим разность числа нормалей разделяющих плоскостей, направленных на блок, и тех, что направлены от блока, $\alpha_i, i=\overline{1,n}$. Нетрудно понять, что для '+'-блоков $\alpha_i \ge 0$, для '-'-блоков $\alpha_i \le 0$, причем не может быть одновременного равенства нулю. Более того, $\alpha_i$ для '+' равны между собой и на 2 больше, чем $\alpha_i$ для '-'. Поэтому простая композиция $n-1$ decision stump алгоритмов дает классификацию без ошибок на тестовой выборке.\\

Почему не может быть меньше алгоритмов, скажем, $k < n$? Потому что тогда блоков будет меньше и некоторые будут содержать разные метки, поэтому на объектах такого блока алгоритм будет ошибаться.

\subsection{Задача}
\begin{enumerate}[label=\alph*]
	\item В качестве базового классификатора берем выделение произвольной прямоугольной области. Берем 12 таких штучек.
	\item Базовый классификатор $x^2 + y^2 <[>] R^2$. Берем таких 3, сколько границ между синим и красным, направляем внутренность
	в красную область. Тогда красные области по -1, синие - по +1.
	\item Здесь можно обойтись двумя SVM-ами примерно.
	\item Здесь можно четырьмя decision stump'ами, должны смотреть внутрь синего прямоугольника. Критерий синюшности - набираем 4 балла.
\end{enumerate}

\section{Соревнование}


\end{document} 